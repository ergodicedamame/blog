[
  {
    "objectID": "posts/bayesian-model-comparison/index.html",
    "href": "posts/bayesian-model-comparison/index.html",
    "title": "Bayesian comparison of cross-validated algorithms",
    "section": "",
    "text": "Robust model evaluation should be second nature for those of us that work with data and predictive models. When determining whether one model is better than another, there are many techniques that one can use. Very common ones include bootstrapping and cross-validation.\nFirst and foremost however, one must make a decision as to what “better” means. Better in what way? More accurate? Faster? More (budget) efficient? We will revisit this later. For now, let’s assume that better means “obtains a higher F1 score”. F1 is just an example, and we could consider any other score. Once we have decided to say that the model with the highest F1 score is better, the more serious question arises: how do we know whether a model is indeed better? Can we just take an average score across a test set and compare the numbers, i.e. plain-old pointwise comparison? I think we agree that we would like something more robust and which, ideally, takes uncertainty into account."
  },
  {
    "objectID": "posts/bayesian-model-comparison/index.html#problem-statement",
    "href": "posts/bayesian-model-comparison/index.html#problem-statement",
    "title": "Bayesian comparison of cross-validated algorithms",
    "section": "Problem statement",
    "text": "Problem statement\nAssume that we are interested in comparing the performance (e.g. accuracy, recall, precision, \\(\\text{F}_{\\beta}\\), etc.) of two predictors. If using cross-validation, Bouckaert (2003) recommends making this more robust (i.e. returning more stable statistics) by using repeated cross-validation; that is performing evaluation via \\(m\\) repetitions of \\(k\\)-fold cross-validation. Both classifiers will be trained on the same data and tested on the same data and we can therefore collect the differences of their performances\n\\[\n\\delta = \\{\\delta_{1}, \\delta_{2},. . ., \\delta_{n}\\}\n\\]\nwhere \\(n=mk\\). Denote the sample mean and sample variance of the differences as \\(\\hat{\\delta}\\) and \\(\\hat{\\sigma}^{2}\\).\nWhat can we say about these samples? Well, one thing is that these samples are almost certainly noisy observations of an underlying true difference of performance between the two algorithms. That is, there exist a hidden performance difference which we don’t know but that manifest itself, with noise, through these samples. For instance, if we assume that the \\(\\delta_{i}\\) s are i.i.d. then we could say that the \\(\\delta_{i}\\) are samples from a distribution with a true underlying mean, which would be the mean of interest to us. We could then further assign a prior distribution to that true mean (and any other parameters in the specification of the sampling distribution) and, after observing some samples, perform inference to construct the posterior distribution over the true mean, given the observed data. This is the standard paradigm of Bayesian modelling.\nThere are infinite ways to model such a process but we can follow the aphorism, gifted to us by George Box, that none of them will be correct, but some of them may be useful. In particular, the assumtions that we bake into the model will make the model more or less useful. Oversimplifying assumptions would lead us to a model that is not able to describe the underlying process. Complex interactions would lead to a model that is intractable. Of course these days, with powerful probabilist programming languages, it is much easier to build a complex model and have a black-box inference algorithm that will give us results. This, however, isn’t a free meal. Complex models, especially hierachical ones, lead to complex inference processes that require expertise to diagnose. Moreover, the choice of priors will also become a delicate issue.\nIn general, we need to balance two things:\n\nthe modelling assumptions,\nthe techniques to perform inference, i.e. the computational techniques.\n\nIn the case that we are considering here, i.e. comparing models across cross-validation scores, one important modelling aspect is that the scores are not independent: in fact the resulting models will have an overlapping training set and an overlapping testing set.\nIn this post, we will be presenting a model that was developed in Corani and Benavoli (2015). This is a relatively simple model which uses the properties of exponential families and conjugacy to simplify inference. Note that this is just one possible model of the process, and it is by no means the best model!"
  },
  {
    "objectID": "posts/bayesian-model-comparison/index.html#corani-and-benavolis-bayesian-correlated-t-test",
    "href": "posts/bayesian-model-comparison/index.html#corani-and-benavolis-bayesian-correlated-t-test",
    "title": "Bayesian comparison of cross-validated algorithms",
    "section": "Corani and Benavoli’s Bayesian correlated t-test",
    "text": "Corani and Benavoli’s Bayesian correlated t-test\nThe model proposed by Corani and Benavoli (2015) is as follows. Assume that the observations \\(\\delta_i\\), are identically distributed but dependent. Specifically, let the observations have the same mean, \\(\\mu\\), the same precision, \\(\\nu,\\) and be equally correlated with each other with correlation \\(\\rho > 0\\). This is the case when the \\(n\\) observations are the \\(n\\) differences of performance between two predictors yielded by cross-validation. The data generation process is then modelled as:\n\\[\n\\delta = I_{n \\times 1}\\mu + v\n\\]\nwhere \\(v\\) is a noise vector with zero mean and covariance matrix \\(\\Sigma_{n \\times n}\\) with the following structure: each diagonal element equals \\(\\sigma^{2} = 1/\\nu\\); non-diagonal elements equal \\(\\rho \\sigma^{2}\\). This is knows as the interclass covariance matrix.\nDefine \\(\\Sigma = \\sigma^{2} M\\) with \\(M\\) an \\((n \\times n)\\) correlation matrix, e.g. in \\(3d\\)\n\\[\nM =\n  \\left[ {\\begin{array}{cc}\n    1 & \\rho & \\rho \\\\\n    \\rho & 1 & \\rho\\\\\n    \\rho & \\rho & 1\\\\\n  \\end{array} } \\right]\n\\]\nNadeau and Bengio (2003) show that the correlation between the cross-validation results is positive and therefore, since \\(\\sigma^{2} > 0\\), \\(\\Sigma\\) is invertible and positive definite.\nWith this we have the basic ingredient indicating how we want to model the interactions between the samples, however we still need to define a generative process; a prior over \\(\\mu\\) (the quantity of interest) and a likelihood (i.e. a sampling distribution). For the sampling distribution of correlated observations assume the noise vector, \\(v\\), to be distributed as a multivariate Normal distribution:\n\\[\nP(\\delta| \\mu, \\Sigma) = \\frac{\n  \\exp(-\\frac{1}{2}(\\delta - I\\mu)^{T}\\Sigma^{-1}(\\delta - I\\mu))}\n  {(2\\pi)^{n/2}\\sqrt{|\\Sigma|}}\n\\]\nWe ultimately want a distribution over \\(\\mu\\). We could go about this by defining priors over the three parameters, \\(\\mu\\), \\(\\sigma^{2}\\) and \\(\\rho\\), and inferring the posterior over these. The approach taken by the authors here is simpler and, importantly, allows us to get the posterior in closed form. This is achieved by avoiding the need to estimate \\(\\rho\\) through use of the Nadeau-Bengio heuristic estimate for the correlation.\nSpecifically Nadeau and Bengio propose to take \\(\\rho = \\frac{n_{\\text{test}}}{n_{\\text{total}}}\\), where \\(n_{\\text{test}}\\) is the size of the test set and \\(n_{\\text{total}} = n_{\\text{test}} + n_{\\text{train}}\\), i.e. the total dataset size.\n\n\n\n\n\n\nThe idea behind the choice of correlation\n\n\n\n\n\nSee Nadeau and Bengio (2003) sections 3 and 5 for details. The main points and heuristics behind this choice for the correlation factor are:\n\nThe real \\(\\rho\\) is unknown, in fact usually this is just set to \\(0\\) meaning that correlation is not accounted for. This can lead to underestimates of the estimated variance. In the paper the authors propose \\(\\rho=\\frac{n_{\\text{test}}}{n_{\\text{total}}}\\). Note that this choice, as stated by the authors, is a gross approximation. Yet it is better than pretending that \\(\\rho=0\\).\nThe choice is made with an important assumption – that the specific training instances don’t matter, just the training set size. This is a very strong assumption and one that isn’t generically justified.\nThis correction is good for stable algorithms (c.f. the point above), i.e. ones that are not too sensitive to perturbation of the training set. Or for algorithms with low capacity compared to training set size.\nThe idea is that the correlation should reduce as more training data is used. More training data should stabilise the algorithm. That is, as the data size increases the model should approach saturation and, therefore, as we keep adding data the resulting decision function shouldn’t change too much.\n\n\n\n\nOkay, so now we’re in business! With \\(\\rho\\) fixed we can infer the posterior over \\(\\mu\\).\nChoose the joint prior for the mean and precision parameters of the Normal distribution to be\n\\[P(\\mu, \\nu | \\mu_{0}, k_{0}, a, b) = NG(\\mu, \\nu; \\mu_{0}, k_{0}, a, b)\n\\]\nwhere \\(NG\\) is the standard Normal-Gamma. This is a conjugate prior, to the normal, which makes inference easier.\nBecause of the conjugacy, the posterior over \\(\\{\\mu, \\nu\\}\\) will also be a Normal-Gamma, i.e.\n\\[\nP(\\mu, \\nu| \\delta, \\mu_{0}, k_{0}, a, b, \\rho) = NG(\\mu, \\nu; \\tilde{\\mu}_{n}, \\tilde{k}_{n}, \\tilde{a}_{n}, \\tilde{b}_{n})\n\\]\nWe are interested in the distribution over \\(\\mu\\). In order to obtain that, we have to marginalise over \\(\\nu\\). Doing so results in a Student’s t-distribution for \\(\\mu\\) (2015)\n\\[\nP(\\mu|\\delta, \\mu_{0}, k_{0}, a, b, \\rho) = St(\\mu; 2\\tilde{a}_{n}, \\tilde{\\mu}_{n}, \\frac{\\tilde{b}_{n}\\tilde{k}_{n}}{\\tilde{a}_{n}})\n\\]\nThe expression for the parameters of the Student distribution are a little unwieldy, however if we use what is called a matching prior2 then this is simplified. We use the matching prior given by \\(\\mu_{0} = 0\\), \\(k_{0} \\rightarrow \\infty\\), \\(a = -1/2\\) and \\(b = 0\\) to get:2 A matching prior is a prior for which posterior probability statements about the parameter also have an interpretation as confidence statements in the sampling model; i.e. the posterior will return properties that match the frequentist’s analysis.\n\\[\nSt(\\mu; n - 1, \\hat{\\delta}, (\\frac{1}{n} + \\frac{\\rho}{1 - \\rho})\\hat{\\sigma}^{2})\n\\]\nwhere \\(\\hat{\\delta} = \\frac{\\Sigma_{i = 1}^{n} \\delta_{i}}{n}\\) and \\(\\hat{\\sigma}^{2} = \\frac{\\Sigma_{i = 1}^{n}(\\delta_{i} - \\hat{\\delta})^{2}}{n - 1}\\)"
  },
  {
    "objectID": "posts/bayesian-model-comparison/index.html#time-to-code-this-up",
    "href": "posts/bayesian-model-comparison/index.html#time-to-code-this-up",
    "title": "Bayesian comparison of cross-validated algorithms",
    "section": "Time to code this up!",
    "text": "Time to code this up!\n\nfrom time import perf_counter\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\nTo see this in action, the first thing we need is some data and repeated cross-validation predictions for two different algorithms.\nWe use sklearn to make a moons dataset:\n\nX, y = make_moons(n_samples=600, noise=0.4, random_state=23)\n\nThis has an equal number of labels of each class.\nNow that we have the data, let’s run the repeated cross-validation for two models. Here we use a Random Forest and a neural network (MLPClassifier) classifier:\n\nn_repetitions = 10\nn_folds = 5\n\nRSKF = RepeatedStratifiedKFold(\n  n_splits=n_folds, n_repeats=n_repetitions, random_state=23\n)\n\nmodel_1 = \"RF\"\nmodel_2 = \"MLP\"\nmodels = {\n  model_1: RandomForestClassifier(),\n  model_2: MLPClassifier(alpha=1, max_iter=1_000)\n}\n\nmodel_scores = {}\nmodel_times = {}\n\n# * loop over the models\nfor model_name, model in models.items():\n  times_fit = []\n  scores = []\n  # * loop over the repeated-cv folds\n  for train_indices, test_indices in RSKF.split(X, y):\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    \n    # * fit model and time fitting process\n    time_start = perf_counter()\n    clf = model.fit(X_train, y_train)\n    time_end = perf_counter()\n    time_fit = time_end - time_start\n\n    # * get model prediction\n    y_pred = clf.predict(X_test)\n\n    # * compute f1 score\n    f1 = f1_score(y_test, y_pred)\n\n    times_fit.append(time_fit)\n    scores.append(f1)\n  \n  model_scores[model_name] = np.asarray(scores)\n  model_times[model_name] = {\"fit\": times_fit}\n\nNow that we have the scores for each model, let’s perform the test. We’ll first need to compute the array of differences, \\(\\delta\\), and then we’re in business.\n\n# * create array of differences; we do model_2 (MLP) - model_1 (RF)\ndelta = model_scores[model_2] - model_scores[model_1]\n\n\n\nCode for compute_statistics\ndef compute_statistics(\n  perf_differences, n_repetitions,\n  ):\n  \"\"\"\n  Given the m*k length array holding the scores for the m-repetitions of k-folds, will compute \n  the following statistics: the mean, the Nadeau-Bengio corrected \n  variance and the number of degrees of freedom for the t-distribution.\n  \"\"\"\n  mean = np.mean(perf_differences)\n  variance = np.var(perf_differences, ddof=1)\n  # * Now account for the correlations across measurements with the \n  # * Nadeau-Bengio correction of variance\n  num_of_measurements = perf_differences.size\n  correlation = n_repetitions / num_of_measurements\n  variance *= 1 / num_of_measurements + correlation / (1 - correlation)\n  return mean.item(), variance.item(), num_of_measurements - 1\n\n\n\n# * obtain the relevant statistics\nmean, variance, dof = compute_statistics(\n  perf_differences=delta, n_repetitions=n_repetitions\n)\n\n\n\nt-distribution statistics:\nmean 0.0031, \nvariance 0.00012, \ndegrees of freedom 49.\n\n\nA useful thing we can do is to select a region of practical equivalence, ROPE. This is a region where the difference in performance can be considered practically equivalent, i.e. a difference lying within the ROPE is an inconsequential difference. Clearly the choice of ROPE is subjective and will depend on the metric and the scale we use to compare the algorithms in addition to our understanding of equivalence in the given situation. See Kruske (2013) and Benavoli et al. (2017) for more details.\nHere we will say that a difference of 1% in performance between the two models makes the performance practically equivalent.\n\n\nCode for get_posteriors_from_t_distribution\ndef get_posteriors_from_t_distribution(\n  mean, variance, dof, rope=(0.0, 0.0), precision=4\n):\n  \"\"\"Compute and return probability mass to the left of the given rope, \n  within the given rope and to the right of the given rope for a \n  t-distribution specified by the given mean, variance and degrees \n  of freedom. \n  NB: probabilities are computed from the cumulative Student distribution, not \n  from a sampled posterior.\n  \"\"\"\n  # * Deal with the situation where the variance is very small by assigning entire \n  # * probability mass to the appropriate regions\n  if np.isclose(variance, 0.0):\n    prob_left = float(mean < rope[0])\n    prob_right = float(mean > rope[1])\n  # * Otherwise compute the probability for the specified t-distribution.\n  else: \n    std = np.sqrt(variance)\n    prob_left = stats.t.cdf(rope[0], dof, loc=mean, scale=std)\n    prob_right = 1 - stats.t.cdf(rope[1], dof, loc=mean, scale=std)\n  prob_centre = 1 - prob_left - prob_right\n  return [round(p, precision) for p in [prob_left, prob_centre, prob_right]]\n\n\n\n# * Select a Region of practical equivalence:\nROPE = (-0.01, 0.01)\n\nprob_model_1_better, prob_rope, prob_model_2_better = get_posteriors_from_t_distribution(\n  mean=mean, variance=variance, dof=dof, rope=ROPE\n)\n\n\n\nP(RF > MLP) = 0.1183, \nP(RF ~ MLP) = 0.6162, \nP(RF < MLP) = 0.2655\n\n\nAnd what’s nicer, we can look at the distribution as the matching prior returns a posterior that is a t-distribution:\n\n\n\n\n\nSince the posterior distribution informs us about the relative credibility values across the reals, from the posterior we get the uncertainty in the estimate. From this we can get a whole lot of useful information, for instance the Highest Density Intervals (HDIs), the mode, the mean, etc. Furthermore, equipped with the posterior distribution and the region of practical equivalence we can:\n\nEstimate the posterior probability of a reasonable null hypothesis, i.e. if the difference in performance is within a couple of percentage points they may well be considered equivalent. This will be given by the area within the rope region, above denoted by \\(P(RF \\sim MLP | D)\\).\nEstimate the posterior probability that one model is better than the other, i.e. \\(P(RF > MLP | D)\\) and \\(P(RF < MLP | D)\\). These will be given by the areas on either side of the ROPE.\nRepresent effect size and uncertainty."
  },
  {
    "objectID": "posts/bayesian-model-comparison/index.html#cost-sensitive-decisions",
    "href": "posts/bayesian-model-comparison/index.html#cost-sensitive-decisions",
    "title": "Bayesian comparison of cross-validated algorithms",
    "section": "Cost sensitive decisions",
    "text": "Cost sensitive decisions\nWhile we will be dealing with this in more detail in a separate blog post, let’s take a first stab.\nIn the real world, it is usually the case that we want to reason and make decisions about situations based on the concept of cost. The choice of the cost measure should depend on how the system is going to be used, rather than on any inherent specification of the training process. The issue with doing this is that it is hard. It is hard because specifying a cost-aware loss function is non-trivial, because cost-specifications are domain specific, and because even in the case of roughly knowing what the costs are, using this information is hard, i.e. the specified weigthed cost may be a difficult objective for optimisers to work with. However if we can specify costs then decision making based on these would be the best way to work as this allows one to take into consideration the utility of the decision that will be made.\nIn our situation, in order to do cost-sensitive analysis/decision making, all we need is to specify a cost function – we won’t need to run an optimisation on this objective function. This is a function that defines the loss (or cost) we incur in making a given decision (e.g. the wrong decision). A typical example is whether to give more importance to a false positive or a false negative. For our given situation and for the sake of exposition, let’s assume that we are interested in the time taken to fit the model as we will need to do it often. (Also because in our example, the models actually take the same time to predict.) We find that\n\n\nMedian time to fit RF is 0.1127 s, \nMedian time to fit MLP is 2.5925 s, \nRatio MLP / RF is 23.0\n\n\nThat is, MLP is a lot slower to fit than RF. There are three decisions we can make\n\nRF is better than MLP\nRF is equivalent to MLP\nRF is worse than MLP\n\nWe consider the following cost-matrix:\n\n\n\n\n\n\n\n\n\n\nRF is better\nare equivalent\nMLP is better\n\n\n\n\nChoose RF\n0\n-5\n2\n\n\nChoose MLP\n7\n5\n0\n\n\n\nwhere the \\((i, j)\\)th entry is the cost incurred by making decision \\(i\\) when \\(j\\) is correct. Here we have a \\(2 \\times 3\\) matrix as we only consider the options of selecting either one model or the other, no abstention, or anything else.\nIn this case we have:\n\ncost of choosing the Random Forest is:\n\n0 if it is better,\n-5 if they are equivalent as we save compute time,\n2 if MLP is better, as we would lose performance\n\ncost of choosing the MLPClassifier is:\n\n7 if RF is better because we pay for computational and performance cost,\n5 if they are equivalent as we add to the compute time,\n0 if MLP is indeed better\n\n\nThe expected cost can then be obtained by multiplying the cost matrix with the relevant posterior probabilities. In this case, the relevant probabilities are \\(P(RF > MLP | D)\\), \\(P(RF \\sim MLP | D)\\) and \\(P(RF < MLP | D)\\).\n\ncost_matrix = np.array([\n  [0., -5., 2.],\n  [7., 5., 0.],\n])\n\nprobabilities = np.array([\n  prob_model_1_better, \n  prob_rope, \n  prob_model_2_better\n])\n\nexpected_cost = cost_matrix.dot(probabilities)\n\n\n\n\nCost of deciding on RF: -2.55\n\nCost of deciding on MLP: 3.9091\n\n\nThe lowest expected cost would determine the optimal decision. We see that we would incur a significant cost in choosing MLP over RF. Of course, we could have made things even more extreme by specifying more aggressive costs. And further, we could have extended the possibilities, e.g. adding a row for “no decision made”."
  },
  {
    "objectID": "posts/heatmaps-clustering/index.html",
    "href": "posts/heatmaps-clustering/index.html",
    "title": "Heatmaps done right",
    "section": "",
    "text": "Given a distance matrix, i.e. a symmetric matrix of distances between observations, if the indices of the observations are arbitrary, or more generally there is no variable by which we want to order the observations, then plotting it as a heatmap using the default ordering (of the indices) is often not very useful. If we do so, we get a heatmap where the rows and columns are ordered abritrarily and the plot itself may be hard to interpret. In many cases, a better approach is to cluster the observations and use the new ordering This will lead to a heatmap in which similar observations are grouped. This is what sns.clustermap offers, as seen in the docs.\nNote that the data itself is not the focus here. We just need a distance matrix in order to show how it can be reordered. We will use the USA presidential speeches, and consider distances between texts, to walk through the clustering and then do the plotting using altair for interactivity - as much as we like seaborn (and it’s next generation API looks very cool), interactivity is great! Then again from the source code, it looks like a lot of thought has been put into clustermap, so there might be other reasons to use it."
  },
  {
    "objectID": "posts/heatmaps-clustering/index.html#set-up",
    "href": "posts/heatmaps-clustering/index.html#set-up",
    "title": "Heatmaps done right",
    "section": "Set up",
    "text": "Set up\n\n!pip install -r requirements.txt"
  },
  {
    "objectID": "posts/heatmaps-clustering/index.html#imports",
    "href": "posts/heatmaps-clustering/index.html#imports",
    "title": "Heatmaps done right",
    "section": "Imports",
    "text": "Imports\n\nimport altair as alt\nimport nltk\nfrom nltk.corpus import inaugural\nimport pandas as pd\n\n# from scipy.spatial.distance import pdist\nfrom scipy.cluster import hierarchy"
  },
  {
    "objectID": "posts/heatmaps-clustering/index.html#data",
    "href": "posts/heatmaps-clustering/index.html#data",
    "title": "Heatmaps done right",
    "section": "Data",
    "text": "Data\nLet us quickly get the presidential addresses from nltk, and then compute the Jaccard distance on the 5-shingles1.1 Shingles are substrings of a certain length that are created by passing a moving window along the text. This choice of representation of the texts is common when working with web data, and can deal with mispellings and the sequential nature of text (to some extent). See e.g. (Schütze, Manning, and Raghavan 2008, sec. 19.6).\nFirst we download and import the data.\n\nnltk.download(\"inaugural\")\n\n\nids = inaugural.fileids()\ndata = [\n    {\n        \"id\": i,\n        \"year\": id.split(\"-\")[0],\n        \"president\": (id.split(\"-\")[1]).split(\".\")[0],\n        \"text\": inaugural.raw(fileids=id).lower(),\n    }\n    for i, id in enumerate(ids)\n]\ndf = pd.DataFrame(data)\n\nThen we shingle the text.\n\ndef get_shingles(x, size=5):\n    x = x + (size * \" \")\n    shingles = [x[i : i + size] for i in range(0, len(x) - size)]\n    return shingles\n\n\ndf[\"shingles\"] = df[\"text\"].apply(get_shingles)\n\nAnd finally we can compute the Jaccard similarity.\n\ndef get_similarity(x, y, precision=3):\n    a = set(x)\n    b = set(y)\n    return round(len(a.intersection(b)) / len(a.union(b)), precision)\n\n\ndf_pairs = df.copy()\ndf_pairs[\"key\"] = 0\ndf_pairs = df_pairs.merge(df_pairs, on=\"key\").drop(columns=[\"key\"])\n\ndf_pairs[\"similarity\"] = df_pairs.apply(\n    lambda row: get_similarity(row[\"shingles_x\"], row[\"shingles_y\"]), axis=1\n)\n\ndf_pairs.drop(columns=[\"text_x\", \"text_y\", \"shingles_x\", \"shingles_y\"], inplace=True)\n\n\ndf_pairs.head()\n\n\n\n\n\n  \n    \n      \n      id_x\n      year_x\n      president_x\n      id_y\n      year_y\n      president_y\n      similarity\n    \n  \n  \n    \n      0\n      0\n      1789\n      Washington\n      0\n      1789\n      Washington\n      1.000\n    \n    \n      1\n      0\n      1789\n      Washington\n      1\n      1793\n      Washington\n      0.071\n    \n    \n      2\n      0\n      1789\n      Washington\n      2\n      1797\n      Adams\n      0.231\n    \n    \n      3\n      0\n      1789\n      Washington\n      3\n      1801\n      Jefferson\n      0.219\n    \n    \n      4\n      0\n      1789\n      Washington\n      4\n      1805\n      Jefferson\n      0.218"
  },
  {
    "objectID": "posts/heatmaps-clustering/index.html#clustering",
    "href": "posts/heatmaps-clustering/index.html#clustering",
    "title": "Heatmaps done right",
    "section": "Clustering",
    "text": "Clustering\nClustering is straightforward. We are after the “optimal” ordering, i.e. the re-ordering with similar values placed close to each other.\n\nmat_pairs = df_pairs.pivot(index=\"id_x\", columns=\"id_y\", values=\"similarity\").to_numpy()\n\nZ = hierarchy.linkage(mat_pairs, optimal_ordering=True)\n\nreordering = hierarchy.leaves_list(Z)"
  },
  {
    "objectID": "posts/heatmaps-clustering/index.html#plotting",
    "href": "posts/heatmaps-clustering/index.html#plotting",
    "title": "Heatmaps done right",
    "section": "Plotting",
    "text": "Plotting\n\nBy date\nNote that our indices are actually ordered by date. This might be an interesting dimension and plotting the distances by date might reveal some insight from the data.\n\nw = 500\n\n\n(\n    alt.Chart(df_pairs)\n    .mark_rect()\n    .encode(\n        x=\"id_x:O\",\n        y=\"id_y:O\",\n        color=alt.Color(\"similarity:Q\", scale=alt.Scale(scheme=\"reds\")),\n        tooltip=[\"president_x\", \"year_x\", \"president_y\", \"year_y\", \"similarity\"],\n    )\n    .properties(width=w, height=w)\n    .interactive()\n)\n\n\n\n\n\nFigure 1: Heatmap using default order.\n\n\n\nIndeed, it seems like there are some interesting groups of two or three consecutive terms/presidents with similar inaugural speeches. One could probably have fun checking political party and second terms in office.\nOften however, indices are randomly assigned, in which case there is never a good reason to order by the original indices when plotting.\n\n\nBy distance\n\n(\n    alt.Chart(df_pairs)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"id_x:O\", sort=alt.Sort(reordering)),\n        y=alt.Y(\"id_y:O\", sort=alt.Sort(reordering)),\n        color=alt.Color(\"similarity:Q\", scale=alt.Scale(scheme=\"reds\")),\n        tooltip=[\"president_x\", \"year_x\", \"president_y\", \"year_y\", \"similarity\"],\n    )\n    .properties(width=w, height=w)\n    .interactive()\n)\n\n\n\n\n\nFigure 2: Heatmap using re-ordering from clustering.\n\n\n\nUnfortunately, with our arbitrary choice of data, the reordering doesn’t seem to add much. However, we can still see a single main cluster of similar values that might deserve further inspection and reveal something, as well as at least one other smaller and less homogeneous cluster."
  },
  {
    "objectID": "posts/how-to/index.html",
    "href": "posts/how-to/index.html",
    "title": "Setting up a blog using quarto",
    "section": "",
    "text": "This is a minimal, living document to remind ourselves how we set up the blog, and how to write on it. We will update it as needed. It may turn out to be useful more generally, but the best place to look is the quarto site."
  },
  {
    "objectID": "posts/how-to/index.html#folders-and-files",
    "href": "posts/how-to/index.html#folders-and-files",
    "title": "Setting up a blog using quarto",
    "section": "Folders and files",
    "text": "Folders and files\nThere are a number of files needed for a quarto blog project, as we can see by taking a look at the repository. The main ones are listed here.\nOther files in the repository are needed for quarto to execute the python notebooks, github pages, etc."
  },
  {
    "objectID": "posts/how-to/index.html#github-pages-ci",
    "href": "posts/how-to/index.html#github-pages-ci",
    "title": "Setting up a blog using quarto",
    "section": "Github pages CI",
    "text": "Github pages CI\nThe website is re-built (on pushes) and deployed (on merge into main) using github actions, see workflow."
  },
  {
    "objectID": "posts/how-to/index.html#commenting-functionality",
    "href": "posts/how-to/index.html#commenting-functionality",
    "title": "Setting up a blog using quarto",
    "section": "Commenting functionality",
    "text": "Commenting functionality\nProvided by giscus, which links to github Discussions, see quarto docs and links."
  },
  {
    "objectID": "posts/how-to/index.html#writing-posts",
    "href": "posts/how-to/index.html#writing-posts",
    "title": "Setting up a blog using quarto",
    "section": "Writing posts",
    "text": "Writing posts\n\nAll posts should be stored as .qmd files, indeed .Rmd and .ipynb are included in the .gitignore.\njupytext allows to link .ipnb and .qmd notebooks. This is set up in the .jupytext.toml file. One can also use jupytext --to ipynb index.qmd for individual posts.\nEvery post should have its own folder in the posts/ folder, as outlined in quarto docs.\nQuarto allows to render the blog locally as one works on a new post (or any other changes) using quarto preview. This is live-reloaded whenever we save.\nContents of _freeze should be committed, as the CI does not rebuild the posts. This could lead to posts breaking if dependencies change, etc. and is advised against in the quarto docs.\n\nNOTE: Any rendered content, in the docs/ folder, should not be pushed to the repository, but we cannot (simply) add it to the .gitignore, as this would break the CI which pushes the contents of docs/ to the pages branch."
  },
  {
    "objectID": "posts/discriminative-vs-generative/index.html",
    "href": "posts/discriminative-vs-generative/index.html",
    "title": "Generative vs. discriminative models",
    "section": "",
    "text": "Generative and discriminative models are known to have complementary strenghts. Following Minka (2005) and Lasserre, Bishop, and Minka (2006), we will show how they can be seen to be at different ends of a spectrum, and then discuss some of their differences in detail."
  },
  {
    "objectID": "posts/discriminative-vs-generative/index.html#generative-models-can-be-used-to-generate-synthetic-data",
    "href": "posts/discriminative-vs-generative/index.html#generative-models-can-be-used-to-generate-synthetic-data",
    "title": "Generative vs. discriminative models",
    "section": "Generative models can be used to generate synthetic data",
    "text": "Generative models can be used to generate synthetic data\nA discriminative model is one that provides \\(P \\left( y \\vert x \\right)\\) directly. The name comes from the fact that we can view the distribution (together with the decision to choose the most probable value) as directly discriminating the value of the target \\(y\\) for any given instance \\(x\\).\n\n\n\n\n\nDiscriminative model, as used for prediction.\n\n\n\n\nMirorring the definition of a discriminative model, a generative model is instead often defined as one that provides the joint probability \\(P \\left( x, y \\right)\\) in the form \\(P \\left( x \\vert y \\right)P \\left( y \\right)\\), on which one can then use Bayes’ theorem to obtain \\(P \\left( y \\vert x \\right) \\propto P \\left( x \\vert y \\right)P \\left( y \\right).\\) The graph representation for such a model is shown below.\n\n\n\n\n\nGenerative model with “default” factorization, as used for prediction.\n\n\n\n\nA good reference on classification, which introduces and compares these approaches, is Bishop (2006, chap. 4).\nThe more general definition is that a generative model is one that can generate data via ancestral sampling, i.e. sampling from the priors and passing the sampled values through the graphical model. This includes the definition above, which also has a choice of factorisation. We can also turn our discriminative model into a generative one by adding a factor for \\(P \\left( x \\right)\\), such that it too models the joint distribution \\(P \\left( x, y \\right)\\). Put differently, the factorization used in the definition of the generative model above is not what makes it a generative model. It is rather the fact that it models the joint distribution. In order to distinguish between the (first) generative model and the extended discriminative model, which is also a generative model, Mitchell refers to the former as a Bayes classifier given that it uses Bayes theorem to recover \\(P \\left( y \\vert x \\right)\\) (Mitchell 2020).\n\n\n\n\n\nExtended discriminative model with prior for \\(x\\), as used for prediction.\n\n\n\n\nNote that we sometimes also find people stating that generative models are ones that capture the causal process by which the actual data (\\(D\\)) is generated. While it is true that one might build a generative model by thinking about the causal process, it could be that the causal data generation process requires \\(P \\left( y \\vert x \\right)\\) rather than \\(P \\left( x \\vert y \\right)\\). We therefore distinguish between generative models and generative processes."
  },
  {
    "objectID": "posts/discriminative-vs-generative/index.html#generative-models-consider-a-more-restictive-parameter-space",
    "href": "posts/discriminative-vs-generative/index.html#generative-models-consider-a-more-restictive-parameter-space",
    "title": "Generative vs. discriminative models",
    "section": "Generative models consider a more restictive parameter space",
    "text": "Generative models consider a more restictive parameter space\nLet’s now look at the fundamental difference between the two model types by considering them in all generality, and focusing on their parametrisation as done by Minka (2005).\nWe write the generative model, with parameters, as\n\\[\nP_1 \\left( x, y \\vert \\theta \\right) = P_{11} \\left( x \\vert y, \\theta \\right) P_{12} \\left( y \\vert \\theta \\right).\n\\]\nWe can train the model, i.e. perform inference, to obtain the posterior probability \\(P \\left( \\theta \\vert D \\right)\\) by considering the joint distribution\n\\[\n\\begin{align*}\nP_g \\left( D, \\theta \\right)    &= P_{01} \\left( \\theta \\right) P \\left( D \\vert \\theta \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) \\prod_i P_1 \\left( x_i, y_i \\vert \\theta \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) \\prod_i P_{11} \\left( x_i \\vert y_i, \\theta \\right) P_{12} \\left( y_i \\vert \\theta \\right),\n\\end{align*}\n\\]\nwhere we have used the iid assumption on the data.\n\n\n\n\n\nParametrised generative model with training data plate.\n\n\n\n\nAlternatively, we can use maximum likelihood estimation to find \\(\\hat \\theta\\). The BIASlab couse nicely explains the different approaches with examples (de Vries, Kouw, and Koudahl 2021).\nLet’s now write the discriminative model, with parameters, as \\(P_{21} \\left( y \\vert x, \\theta \\right)\\). In order to compare it with the generative model, we extend the discriminative model by adding a probability over \\(x\\) and a second parameter in order to obtain the joint distribution\n\\[\nP_2 \\left( x, y \\vert \\theta, \\phi \\right) = P_{21} \\left( y \\vert x, \\theta \\right) P_{22} \\left( x \\vert \\phi \\right),\n\\]\nbut consider the same joint distribution by setting\n\\[\nP_{21} \\left( y \\vert x, \\theta \\right) = \\frac{P_1 \\left( x, y \\vert \\theta \\right)}{\\sum_y P_1 \\left( x, y \\vert \\theta \\right)}\n\\]\nand\n\\[\nP_{22} \\left( x \\vert \\phi \\right) = \\sum_y P_1 \\left( x, y \\vert \\phi \\right).\n\\]\nThe parameters \\(\\theta\\) and \\(\\phi\\) are of the same type, but (for now) assumed independent. We can again obtain the posterior distributions for the parameters by considering the joint distribution\n\\[\n\\begin{align*}\nP_d \\left( D, \\theta, \\phi \\right)    &= P_{01} \\left( \\theta \\right) P_{02} \\left( \\phi \\right) P \\left( D \\vert \\theta, \\phi \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) P_{02} \\left( \\phi \\right) \\prod_i P_2 \\left( x_i, y_i \\vert \\theta, \\phi \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) P_{02} \\left( \\phi \\right) \\prod_i P_{21} \\left( y_i \\vert x_i, \\theta \\right) P_{22} \\left( x_i \\vert \\phi \\right) \\\\\n                                &= \\left( P_{01} \\left( \\theta \\right) \\prod_i P_{21} \\left( y_i \\vert x_i, \\theta \\right) \\right) \\left(P_{02} \\left( \\phi \\right) \\prod_i P_{22} \\left( x_i \\vert \\phi \\right) \\right),\n\\end{align*}\n\\]\nand inferring \\(P \\left( \\theta, \\phi \\vert D \\right)\\).\n\n\n\n\n\nParametrised, extended discriminative model with training data plate.\n\n\n\n\nWe note that, due to the independence assumption, estimation of \\(\\theta\\) and \\(\\phi\\) decouples, namely if we use the factorization above to define\n\\[\nP_d \\left( D, \\theta, \\phi \\right) =: P^1 \\left( y, \\theta \\vert x \\right) P^2 \\left( x, \\phi \\right),\n\\]\nthen we see that Bayes’ rule simplifies, that is\n\\[\n\\begin{align*}\nP \\left( \\theta, \\phi \\vert D \\right)   &= \\frac{P_d \\left( D, \\theta, \\phi \\right)}{\\sum_{\\theta, \\phi} P_d \\left( D, \\theta, \\phi \\right)} \\\\\n                                        &= \\frac{P^1 \\left( y, \\theta \\vert x \\right) P^2 \\left( x, \\phi \\right)}{\\sum_{\\theta , \\phi} P^1 \\left( y, \\theta \\vert x \\right) P^2 \\left( x, \\phi \\right)} \\\\\n                                        &= \\frac{P^1 \\left( y, \\theta \\vert x \\right)}{\\sum_\\theta P^1 \\left( y, \\theta \\vert x \\right) } \\frac{P^2 \\left( x, \\phi \\right)}{\\sum_\\phi P^2 \\left( x, \\phi \\right)} \\\\\n                                        &=: P \\left( \\theta \\vert D \\right) P \\left( \\phi \\vert x \\right).\n\\end{align*}\n\\]\n\n\n\n\n\n\nComparison of parameter space considered by models. The generative model only considers the hyperplane \\(\\theta = \\phi.\\)\n\n\n\nThus \\(\\hat \\theta\\) (or equivalently \\(P \\left( \\theta \\vert D \\right)\\)) is unaffected by the estimation of \\(\\hat \\phi\\) and is the same as what we would have obtained by performing inference on the original, non-extended discriminative model.\nWe see that the fundamental difference between the two models is down to the discriminative one considering a larger parameter space without the constraint \\(\\theta = \\phi\\). This reduces the (statistical) bias, but introduces variance.\nInterestingly, there is no need to assume independence of \\(\\theta\\) and \\(\\phi\\). Considering a joint \\(P \\left( \\theta, \\phi \\right)\\) allows us to work with “hybrid” models."
  },
  {
    "objectID": "posts/discriminative-vs-generative/index.html#generative-models-require-more-assumptions",
    "href": "posts/discriminative-vs-generative/index.html#generative-models-require-more-assumptions",
    "title": "Generative vs. discriminative models",
    "section": "Generative models require more assumptions",
    "text": "Generative models require more assumptions\nWe have just shown that the generative model can be seen as considering a reduced parameter space. Furthermore, compared with the discriminative disctribution \\(P_{21} \\left( y \\vert x , \\theta \\right)\\), the joint distribution considered by the generative model is often hard to work with in practice and further simplifying assumptions are often necessary, or preferable, in order to make inference tractable.\nTo understand why generative models require more modelling assumptions, we will consider the case of Boolean inputs \\(x = \\left( x_1, \\cdots, x_n \\right)\\), \\(x_j \\in \\lbrace 0, 1 \\rbrace\\).\nIt can be instructive to update the factor graphs and draw some of the individual components of the input.\n\n\n\n\n\nDiscriminative model with two components of the input vector drawn.\n\n\n\n\n\n\n\n\n\nGenerative model with two components of the input vector drawn.\n\n\n\n\nLet us now look at the parameters necessary for the generative model by first considering the conditional probability table for \\(P\\left( x \\vert y \\right)\\) with \\(x\\) represented as a single vector.\n\n\n\n\n\\(y = 0\\)\n\\(y = 1\\)\n\n\n\\(x = (0, \\cdots, 0)\\)\n\\(\\theta^0_{1}\\)\n\\(\\theta^1_{1}\\)\n\n\n\\(x = (1, \\cdots, 0)\\)\n\\(\\theta^0_{2}\\)\n\\(\\theta^1_{2}\\)\n\n\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\n\n\\(x = (1, \\cdots, 1)\\)\n\\(\\theta^0_{2^n}\\)\n\\(\\theta^1_{2^n}\\)\n\n\n\nWe see that we have \\(2 \\times 2^n = 2^{n + 1}\\) parameters. The (conditional) probability constraints (on the columns) bring this count down to \\(2 \\left( 2^n - 1\\right)\\).\nThe other factor in the generative model, \\(P \\left( y \\right)\\), is not an issue, as we only have one effective parameter given \\(y\\) is a Boolean variable.\nFor the discriminative model, we instead have to consider \\(P \\left( y \\vert x \\right)\\). Here the conditional probability table is flipped.\n\n\n\n\n\\(x = (0, \\cdots, 0)\\)\n\\(\\cdots\\)\n\\(x = (1, \\cdots, 1)\\)\n\n\n\\(y = 0\\)\n\\(\\theta^0_{1}\\)\n\\(\\cdots\\)\n\\(\\theta^0_{2^n}\\)\n\n\n\\(y = 1\\)\n\\(\\theta^1_{1}\\)\n\\(\\cdots\\)\n\\(\\theta^1_{2^n}\\)\n\n\n\nSimply flipping the conditionality, and again using the conditional probability constraints, leads to \\(2^n\\) effective parameters. This is less parameters than those for the generative model. For large \\(n\\), essentially half as many.\nWhat is often done in generative models is to add further simplifying assumptions. In the Naive Bayes classifier for example, we assume each \\(x_i\\) is conditionally independent of all other \\(x_{-i}\\) given \\(y\\). Together with the product rule, this gives\n\\[\nP \\left(x \\vert y \\right) = \\prod_i P \\left( x_i \\vert y \\right).\n\\]\nWe can visualise this more granular factorization of the conditional probability by drawing the factor graph. This time using plate notation.\n\n\n\n\n\nGenerative model with Naive Bayes assumption.\n\n\n\n\nNow, each \\(x_i\\) has its own conditional probability table, which is simply\n\n\n\n\n\\(y = 0\\)\n\\(y = 1\\)\n\n\n\\(x_i = 0\\)\n\\(\\theta^0_{0}\\)\n\\(\\theta^1_{0}\\)\n\n\n\\(x_i = 1\\)\n\\(\\theta^0_{1}\\)\n\\(\\theta^1_{1}\\)\n\n\n\nand the conditional probability constraints bring the number of parameters per input variable from four to two. Thus, overall we have \\(2 n\\) parameters to estimate. This is now less than the \\(2^n\\) of the discriminative model (provided \\(n > 2\\)).\nOn top of the number of parameters that need to be estimated, in order to reliably estimate them, we need to observe each distinct instance multiple times. This is discussed in (Mitchell 2020).\nWe thus can, and often do, introduce futher bias in generative models in order to make them tractable. A consequence of this is that generative models can be less accurate, if they (i.e. the small world model) don’t reflect the large world model3, but (when they do) generative models require less data to train.3 This is Savage’s terminology, as presented by McElreath in Statistical Rethinking: “All statisitcal modelling has these two frames: the small world of the model itself and the large world we hope to deploy the model in.” (McElreath 2020, 19)."
  },
  {
    "objectID": "posts/discriminative-vs-generative/index.html#generative-models-can-deal-with-missing-data",
    "href": "posts/discriminative-vs-generative/index.html#generative-models-can-deal-with-missing-data",
    "title": "Generative vs. discriminative models",
    "section": "Generative models can deal with missing data",
    "text": "Generative models can deal with missing data\nLet’s turn to the often mentioned fact that generative models can deal with missing data. What this means is that they can still make predictions if given a vector of inputs \\(\\hat x = \\left( \\hat x_1, \\cdots, \\hat x_k, \\bar x_{k+1}, \\cdots, \\bar x_n \\right) = \\left( \\hat x_o, \\bar x_m \\right)\\), where \\(\\bar x_m\\) are missing, whereas discriminative models can’t.\nWhen it comes to predicting \\(\\hat y\\) given \\(\\hat x\\) (and \\(D\\)), we need the posterior predictive distribution, namely\n\\[\n\\begin{align*}\nP \\left( \\hat y \\vert \\hat x, D \\right) &= \\int_\\Theta P \\left( \\hat y, \\theta \\vert \\hat x, D \\right) \\mathrm{d} \\theta \\\\\n                                        &= \\int_\\Theta P \\left( \\hat y \\vert \\hat x, \\theta \\right) P \\left( \\theta \\vert D \\right) \\mathrm{d} \\theta,\n\\end{align*}\n\\]\nwhere we assume that the past and future observations are conditionally independent given \\(\\theta\\).\nIn the case of missing inputs, we want to consider\n\\[\n\\begin{align*}\nP \\left( \\hat y \\vert \\hat x_o, \\theta \\right)  &= \\sum_{\\bar x_m} P \\left( \\hat y, \\bar x_m \\vert \\hat x_o, \\theta \\right) \\\\\n                                                &= \\sum_{\\bar x_m} P \\left( \\hat y \\vert \\hat x_o, \\bar x_m , \\theta \\right) P \\left( \\bar x_m \\vert \\hat x_o, \\theta \\right)\n\\end{align*}\n\\]\nand plug this into the posterior predictive distribution.\n\n\n\n\n\nDiscriminative model being used for prediction, with missing inputs \\(\\bar x_m\\). Note that \\(Xo\\), \\(Xm\\) and \\(Y\\) replace \\(\\hat x_o\\), \\(\\bar x_m\\) and \\(\\hat y\\) due to graphviz limitaitons.\n\n\n\n\nIn the case of discriminative models, we have no way of evaluating the necessary probabilities because we only have \\(P_{21} \\left( y \\vert x, \\theta \\right)\\). We therefore cannot obtain \\(P \\left( \\bar x_m \\vert \\hat x_o, \\theta \\right)\\). We would need to instead resort to some form of imputation. This equates to making assumptions about the distribution \\(P \\left( x \\right)\\), which we would instead have if we consider an extended discriminative model. These can indeed deal with missing observations, given they model the full joint distribution, explicitly via \\(P \\left( y \\vert x \\right)\\) and \\(P \\left(x \\right)\\).\n\n\n\n\n\nGenerative model being used for prediction, with missing inputs \\(\\bar x_m\\). Note that \\(Xo\\), \\(Xm\\) and \\(Y\\) replace \\(\\hat x_o\\), \\(\\bar x_m\\) and \\(\\hat y\\) due to graphviz limitaitons.\n\n\n\n\nIn the generative case, we instead have the joint distribution \\(P_1 \\left( x, y \\vert \\theta \\right)\\). We can therefore use Bayes theorem to get \\(P \\left( \\hat y \\vert \\hat x_o, \\bar x_m , \\theta \\right)\\), as we would anyhow for prediction, and then use the joint distribution with the necessary marginalisations to get \\(P \\left( \\bar x_m \\vert \\hat x_o, \\theta \\right)\\).\nWe can also consider more general forms of missing data, including missing labels and missing inputs in the training data. In the case of generative models, we can train them both in an unsupervised way, when we have no labels, and a semi-supervised way, when we have a few labels. In the case of discriminative models, Minka points out that the extended model can also be trained in a semi-supervised fashion (2005). We will cover this in a future post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Passing Thoughts",
    "section": "",
    "text": "Bayesian comparison of cross-validated algorithms\n\n\nModel selection via a Bayesian correlated t-test.\n\n\n\n\nML\n\n\nModel evaluation\n\n\nBayesian\n\n\nCross-validation\n\n\nCost sensitive analysis\n\n\n \n\n\n\n\n19-11-2022\n\n\n\n\n\n\n  \n\n\n\n\nGenerative vs. discriminative models\n\n\nThinking through the differences between generative and discriminative models.\n\n\n\n\nML\n\n\nFactor graphs\n\n\nPGMs\n\n\n \n\n\n\n\n13-06-2022\n\n\n\n\n\n\n  \n\n\n\n\nHeatmaps done right\n\n\nA note on clustering distance matrices before plotting. Think sns.clustermap using altair.\n\n\n\n\nVisualisation\n\n\nHow-to\n\n\n \n\n\n\n\n19-03-2022\n\n\n\n\n\n\n  \n\n\n\n\nSetting up a blog using quarto\n\n\nNotes on how the site was set up. And how to blog using quarto.\n\n\n\n\nHow-to\n\n\n \n\n\n\n\n06-03-2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains assorted notes and posts on ML, in the broadest sense possible, and other random topics. We don’t really have a clear theme or reader in mind, and are not sure it deserves to be called a blog. It might instead turn out to be a good place to organise our thougts and keep some notes, in which case the reader is us in six months time.\nThis site is powered by quarto, github and all the other cool tech they in turn leverage."
  }
]