{
  "hash": "33cc8f7baa2d0808483e68c277490161",
  "result": {
    "markdown": "---\ntitle: Bayesian comparison of cross-validated algorithms\nsubtitle: Model selection via a Bayesian correlated t-test.\ndate: 2022/11/11\nimage: thumb.png\ncategories:\n  - ML\n  - Theory\n  - Model evaluation\n  - Bayes\n  - Bayesian estimation\n  - Cross-validation\nbibliography: ../references.bib\n---\n\n\n\nRobust model evaluation should be second nature for those of us that work with data and \npredictive models. \nWhen determining whether one model is better than another, there are many techniques one can use.\nVery common ones include *bootstrapping* and *cross-validation*. \n\nFirst and foremost however, one must make a decision as to what \"better\" means.\nBetter in what way? \nMore accurate? \nFaster? \nMore (budget) efficient? \nWe will come to this point again later.\nFor now, let's assume that better means \"obtains a higher F1 score\".\nF1 is just an example, and we could consider any other score. \nOnce we have decided to say that the model with highest F1 score is better, the more serious question arises: how do we know whether a model is indeed better? \nCan we just take an average score across a test set and compare the numbers, i.e. plain-old pointwise comparison? \nI think many will get a sense of discomfort just uttering that. \nWe would like something more robust and which, ideally, takes uncertainty into account. \n\nWe ideally want to have a probability distribution which informs us, in some way, what the probability of one model being better than the other is. \nThat is, we want something like $P(M_1 > M_2 | D)$, where $M_1 > M_2$ denotes that model $M_1$ is better than model $M_2$ and $D$ is the data used for the comparison.\nIn fact, we may want to do more than that. \nIdeally, we would want a distribution specifying what the probability of one model being better than another is for each given performance difference. \nThat is, we would like a distribution over the true performance difference of the two algorithms.\nWhat a great motive to enter the wonderful world of *Bayesian modelling*[^1]. \n\n[^1]: See, e.g. @kruske2013best, for a nice, rapid tour of Bayesian modelling for group comparisons.\n\n## Problem statement\n\nAssume that we are interested in comparing the performance (accuracy, recall, precision, $\\text{F}_{\\beta}$, etc.) of two predictors.\nIf using cross-validation, @bouckaert2003choosing recommends making this more robust (i.e. return more stable statistics) by using *repeated cross-validation*; that is performing evaluation via $m$ repetitions of $k$-fold cross-validation. \nBoth classifiers will be trained on the same data and tested on the same data and we can therefore collect the *differences of their performances* \n\n$$ \n\\delta = \\{\\delta_{1}, \\delta_{2},. . ., \\delta_{n}\\}\n$$ \n\nwhere $n=mk$. Denote the sample mean and sample variance of the differences as $\\hat{\\delta}$ and $\\hat{\\sigma}^{2}$.\n\nWhat can we say about these samples? Well, one thing is that these samples are almost certainly *noisy* observations of an underlying true difference of performance between the two algorithms. \nThat is, there exist a hidden performance difference which we don't know but that manifest itself, with noise, through these samples. \nFor instance, if we assume that the $\\delta_{i}$ s are i.i.d. then we could say that the $\\delta_{i}$ are samples from a distribution with a true underlying mean, which would be the mean of interest to us.\nWe could then further assign a prior distribution to that true mean (and any other parameters in the specification of the sampling distribution) and, after observing some samples, perform inference to construct the posterior distribution over the true mean, given the observed data.\nThis is the standard paradigm of Bayesian modelling.\n\nThere are infinite ways to model such a process but we can follow the aphorism, gifted to us by [George Box](https://en.wikipedia.org/wiki/All_models_are_wrong_), that none of them will be correct, but some of them may be useful.\nIn particular, the assumtions that we bake into the model will make the model more or less useful. \nOversimplifying assumptions would lead us to a model that is not able to describe the underlying process.\nComplex interactions would lead to a model that is intractable. \nOf course these days, with powerful probabilist programming languages, it is much easier to build a complex model and have a black-box inference algorithm that will give us results. \nHowever, this isn't a free meal.\nComplex models, especially hierachical ones, lead to complex inference processes that require expertise to diagnose.\nMoreover, the choce of priors will also become a delicate issue.\n\nIn general, we need to balance two things:\n\n1) the modelling assumptions\n2) the techniques to perform inference, i.e. the computational techniques\n\nIn the case we are considering here, i.e. comparing models across cross-validation scores, one important modelling aspect is that the scores are not independent: in fact the resulting models will have an overlapping training set and an overlapping testing set.\n\nIn this blog we will be presenting a model that was developed in @corani2015bayesian.\nThis is a relatively simple model which uses the powerful modelling techniques of exponential families and conjugacy to simplify inference.\nNote that this is just *one* possible model of the process, and it is by no means the best model!\n\n## Corani and Benavoli's Bayesian correlated t-test\n\nThe model proposed by @corani2015bayesian is as follows.\nAssume that the observations $\\delta_i$, are **identically distributed but *dependent***. \nSpecifically, let the observations have the same mean, $\\mu$, the same precision, $\\nu$, and be equally correlated with each other with correlation $\\rho > 0$. \nThis is the case when the $n$ observations are the $n$ differences of performance between two predictors yielded by cross-validation. \nThe data generation process is then modelled as:\n\n$$\n\\delta = I_{n \\times 1}\\mu + v\n$$\n\nwhere $v$ is a noise vector with zero mean and covariance matrix $\\Sigma_{n \\times n}$ with the following structure: each diagonal element equals $\\sigma^{2} = 1/\\nu$; non-diagonal elements equal $\\rho \\sigma^{2}$. \nThis is knows as the *interclass covariance matrix*. \n\nDefine $\\Sigma = \\sigma^{2} M$ with $M$ an $(n \\times n)$ correlation matrix, e.g. in $3d$\n\n$$\nM =\n  \\left[ {\\begin{array}{cc}\n    1 & \\rho & \\rho \\\\\n    \\rho & 1 & \\rho\\\\\n    \\rho & \\rho & 1\\\\\n  \\end{array} } \\right]\n$$\n\n\n@nadeaubengio2003 show that the correlation between the cross-validation results is positive and therefore, since $\\sigma^{2} > 0$, $\\Sigma$ is invertible and positive definite.\n\nWe have the basic setup but we will need to define a generative process; a prior over $\\mu$ (the quantity of interest) and a likelihood (i.e. a sampling distribution). \nFor the sampling distribution of correlated observations assume the noise vector, $v$, to be distributed as a multivariate Normal distribution:\n\n$$\nP(\\delta| \\mu, \\Sigma) = \\frac{\n  \\exp(-\\frac{1}{2}(\\delta - I\\mu)^{T}\\Sigma^{-1}(\\delta - I\\mu))}\n  {(2\\pi)^{n/2}\\sqrt{|\\Sigma|}}\n$$\n\n\nWe ultimately want a distribution over $\\mu$. \nWe could go about this by defining priors over the three parameters, $\\mu$, $\\sigma^{2}$ and $\\rho$, and inferring the posterior over these. \nThe approach taken by the authors here is simpler and, importantly, allows us to get the posterior in closed form. \nThis is achieved by avoiding the need to estimate $\\rho$ through use of the Nadeau-Bengio heuristic estimate for the correlation. \n\nSpecifically Nadeau and Bengio propose to take $\\rho = \\frac{n_{\\text{test}}}{n_{\\text{total}}}$, where $n_{\\text{test}}$ is the size of the test set and $n_{\\text{total}} = n_{\\text{test}} + n_{\\text{train}}$, i.e. the total dataset size.\n\n:::{.callout-note collapse=\"true\"}\n## The idea behind the choice of correlation \n\nSee @nadeaubengio2003 sections 3 and 5 for details. \nThe main points and heuristics behind this choice for the correlation factor are:\n\n* The real $\\rho$ is unknown, in fact usually this is just set to $0$ meaning that correlation is not accounted for.\nThis can lead to underestimates of the estimated variance. \nIn the paper the authors propose $\\rho=\\rho_{0}=\\frac{n_{\\text{test}}}{n_{\\text{total}}}$. \nNote that this choice, as stated by the authors, is a gross approximation. \nYet it is better than pretending that $\\rho=0$.\n\n* The choice is made with an important assumption -- that the specific training instances don't matter, just the training set size. \nThis is a very strong assumption and one that isn't generically justified.\n\n* This correction is good for stable algorithms (*c.f.* the point above), i.e. ones that are not too sensitive to perturbation of the training set. Or for algorithms with low capacity compared to training set size.\n\n* The idea is that the correlation should reduce as more training data is used. \nMore training data should stabilise the algorithm. \nThat is, as the data size increases the model should approach saturation and, therefore, as we keep adding data the resulting decision function shouldn't change too much.\n:::\n\nOkay, so now we're in business! With $\\rho$ fixed we can infer the posterior over $\\mu$.\n\nChoose the joint prior for the mean and precision parameters of the Normal distribution to be\n\n$$P(\\mu, \\nu | \\mu_{0}, k_{0}, a, b) = NG(\\mu, \\nu; \\mu_{0}, k_{0}, a, b)\n$$\n\nwhere $NG$ is the standard Normal-Gamma.\nThis is a [*conjugate prior*](https://en.wikipedia.org/wiki/Normal-gamma_distribution), to the normal, which makes modelling easier. \n\nBecause of the conjugacy, the posterior over $\\{\\mu, \\nu\\}$ will be a Normal-Gamma also, i.e.\n\n$$\nP(\\mu, \\nu| \\delta, \\mu_{0}, k_{0}, a, b, \\rho) = NG(\\mu, \\nu; \\tilde{\\mu}_{n}, \\tilde{k}_{n}, \\tilde{a}_{n}, \\tilde{b}_{n})\n$$\n\n\nWe are interested in the distribution over $\\mu$. In order to obtain that, we have to marginalise over $\\nu$. Doing so results in a Student's t-distribution for $\\mu$  [-@corani2015bayesian]\n\n$$\nP(\\mu|\\delta, \\mu_{0}, k_{0}, a, b, \\rho) = St(\\mu; 2\\tilde{a}_{n}, \\tilde{\\mu}_{n}, \\frac{\\tilde{b}_{n}\\tilde{k}_{n}}{\\tilde{a}_{n}})\n$$\n\n\nThe expression for the parameters of the Student distribution are a little unwieldy, however if we use what is called a *matching prior*[^2] then this is simplified.\nWe use the matching prior given by $\\mu_{0} = 0$, $k_{0} \\rightarrow \\infty$, $a = -1/2$ and $b = 0$ to get:\n\n$$\nSt(\\mu; n - 1, \\hat{\\delta}, (\\frac{1}{n} + \\frac{\\rho}{1 - \\rho})\\hat{\\sigma}^{2})\n$$\n\nwhere $\\hat{\\delta} = \\frac{\\Sigma_{i = 1}^{n} \\delta_{i}}{n}$ and $\\hat{\\sigma}^{2} = \\frac{\\Sigma_{i = 1}^{n}(\\delta_{i} - \\hat{\\delta})^{2}}{n - 1}$\n\n[^2]: A *matching prior* is a prior for which posterior probability statements about the parameter also have an interpretation as confidence statements in the sampling model; i.e. the posterior will return properties that match the frequentist's analysis.\n\n## Time to code this up!\n\nTo see this in action the first thing we need is some data and repeated cross-validation predictions for two different algorithms.\nWe use the *breast cancer* dataset from *sklearn*:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata = load_breast_cancer()\ntarget = data[\"target\"]\n```\n:::\n\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of instances in total = 569\nOf which num_pos = 357 and num_neg = 212\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntarget_names = [\"malignant\", \"benign\"]\nX_cancer, Y_cancer = data.data, data.target\n```\n:::\n\n\nNow that we have the data let's run the repeated cross-validation for two models, here we use an SVM with linear kernel and a Random Forest classifier:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nn_repetitions = 10\nn_folds = 5\n\nRSKF = RepeatedStratifiedKFold(\n  n_splits=n_folds, n_repeats=n_repetitions, random_state=23\n)\n\nmodels = {\"svm\": SVC, \"rf\": RandomForestClassifier}\n\nmodel_scores = {}\n\nfor model_name, model in models.items():\n  scores = []\n  for i, (train_indices, test_indices) in enumerate(RSKF.split(X_cancer, Y_cancer)):\n    x_train, x_test = X_cancer[train_indices], X_cancer[test_indices]\n    y_train, y_test = Y_cancer[train_indices], Y_cancer[test_indices]\n    \n    clf = model().fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n\n    f1 = f1_score(y_test, y_pred)\n\n    scores.append(f1)\n  \n  model_scores[model_name] = np.asarray(scores)\n```\n:::\n\n\nNow that we have the scores for each model let's perform the test. \nWe'll first need to compute the array of differences, $\\delta$ and then we're in business.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# * create array of differences; we do rf - svm\ndelta = model_scores[\"rf\"] - model_scores[\"svm\"]\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"code for `compute_statistics`\"}\ndef compute_statistics(\n  perf_differences, n_repetitions,\n  ):\n  \"\"\"\n  Given the m*k length array holind m-repetitions of k-fols, will compute \n  the following statistics: the mean, the Nadeau-Bengio corrected \n  variance and the number of degrees of freedom for the t-distribution.\n  \"\"\"\n  mean = np.mean(perf_differences)\n  variance = np.var(perf_differences, ddof=1)\n  # * Now account for the correlations across measurements with the \n  # * Nadeau-Bengio correction of variance\n  num_of_measurements = perf_differences.size\n  correlation = n_repetitions / num_of_measurements\n  variance *= 1 / num_of_measurements + correlation / (1 - correlation)\n  return mean.item(), variance.item(), num_of_measurements - 1\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# * obtain the relevant statistics\nmean, variance, dof = compute_statistics(\n  perf_differences=delta, n_repetitions=n_repetitions\n)\n```\n:::\n\n\nOne useful thing we can do is to select a *region of practical equivalence, ROPE*. \nThis is a region where the difference in performance can be considered *practically equivalent*, i.e. a difference lying within the ROPE is an inconsequential difference. \nClearly the choice of ROPE is subjective and will depend on the *metric* and the *scale* we use to compare algorithms in addition to our subjective understanding of equivalence in the given situation. See @kruske2013best and @benavoli2017timeforachange for more details.\n\nHere we will say that a difference of 1% in performance between the two models makes the performance practically equivalent.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"code for `get_t_dist_posterior_probs`\"}\ndef get_t_dist_posterior_probs(\n  mean, variance, dof, rope = (0.0, 0.0)\n):\n  \"\"\"Compute and return probability mass to the left of the given rope, \n  within the given rope and to the right of the given rope for a \n  t-distribution specified by the given mean, variance and degrees \n  of freedom. \n  NB: probabilities are computed from the cumulative Student distribution, not \n  from a sampled posterior.\n  \"\"\"\n  # * Deal with the situation where the variance is very small by assigning entire \n  # * probability mass to the appropriate regions\n  if np.isclose(variance, 0.0):\n    prob_left = float(mean < rope[0])\n    prob_right = float(mean > rope[1])\n  # * Otherwise compute the probability for the specified t-distribution.\n  else: \n    std = np.sqrt(variance)\n    prob_left = scipy.stats.t.cdf(rope[0], dof, loc=mean, scale=std)\n    prob_right = 1 - scipy.stats.t.cdf(rope[1], dof, loc=mean, scale=std)\n  return prob_left, 1 - prob_left - prob_right, prob_right\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# * Select a Region of practical equivalence:\nROPE = (-0.01, 0.01)\n\nprob_svm_better, prob_rope, prob_rf_better = get_t_dist_posterior_probs(\n  mean=mean, variance=variance, dof=dof, rope=ROPE\n)\n```\n:::\n\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nProb(svm > rf) = 0.0, \nProb(rf ~ svm) = 0.0059, \nProb(svm < rf) = 0.9941\n```\n:::\n:::\n\n\nAnd what's nicer we can look at the distribution as the matching prior returns a t-distribution:\n\n::: {.cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=768 height=515}\n:::\n:::\n\n\nSince the posterior distribution informs us about the relative credibility values across the reals, from the posterior we get the uncertainty in the estimate.\nFrom this we can get a whole lot of useful information, for instance the Highest Density Intervals (HDIs), the mode, the mean, etc.\nFurthermore, equipped with the posterior distribution and the region of practical equivalence we can:\n\n1) estimate the posterior probability of a reasonable null hypothesis, i.e. if the difference in performance is within a couple of percentage points they may well be considered equivalent. \nThis will be given by the area within the rope region, above denoted by $P(M_1 \\sim M_2)$.\n2) significant differences also have practical meaning (area outside the ROPE)\n3) represents effect size and uncertainty (HDI)\n4) we can make meaningful decision based on requiring that the probability of a model being better than another has to be at least 95%, say.\n\n## Cost sensitive decisions\nWe will be dealing with this in more detail in a separate blog post. \nIn the real world, it is usually the case that we want to reason about situations and make decisions based on costs and errors. \nPhrased differently, the choice of the cost measure should depend on how the system is going to be used, rather than on any inherent specification of the training process.\nThe issue with doing this is that it is hard. \nIt is hard because specifying a cost-aware loss function is non-trivial, because cost-specifications are domain specific, and becuase even in the case of roughly knowing what the costs are, using this information is hard, i.e. the specified weigthed cost may be a difficult objective for optimisers to work with.\nHowever if we can specify costs then decision making based on these would be the best way to work as this allows one to take into consideration the utility of the decision that will be made.\n\nIn our situation, in order to do cost-sensitive analysis/decision making, all we need is to specify a cost function -- we won't need to run an optimisation on this objective function. \nThis is a function that defines the loss (or cost) we incur in making a given decision (e.g. the wrong decision).\nA typical example is whether to give more importance to a false positive or a false negative.\nFor our given situation and for the sake of exposition let's assume that the random forest model takes 4 times longer to train. \nThere are three decisions we can make \n\n1) SVM is better than RF\n2) SVM is equivalent to RF\n3) SVM is worse than RF\n\nConsider the following cost-matrix:\n\n+---------------+---------------+--------------------+--------------------+\n|               | SVM is better | are equivalent     | RF is better       |\n+===============+===============+====================+====================+\n| Choose SVM    | 0             | -5                 | 2                  |\n|               |               |                    |                    |\n+---------------+---------------+--------------------+--------------------+\n| Choose RF     | 7             | 5                  | 0                  |\n|               |               |                    |                    |\n+---------------+---------------+--------------------+--------------------+\n\nwhere the $(i, j)$th entry is the cost incurred in making decision $i$ when $j$ is correct.\nHere we have a $2 \\times 3$ matrix as we only consider the options of selecting the SVM or the RF, no abstention or anything else.\n\nIn this case we have:\n\n* cost of choosing SVM is:\n  + 0 if SVM is better, \n  + -5 if they are equivalent as we save compute time, \n  + 2 if RF is better as we would lose performance\n\n* cost of choosing RF is:\n  + 7 if SVM is better because we pay for computational and performance cost, \n  + 5 if they are equivalent as we add to the compute time, \n  + 0 if RF is indeed better\n\n\nThe **expected cost** can then be obtained by multiplying the cost_matrix with the posterior probabilities\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ncost_matrix = np.array([\n  [0., -5., 2.],\n  [7., 5., 0.],\n])\n\nprobabilities = np.array([prob_svm_better, prob_rope, prob_rf_better])\n\nexpected_cost = cost_matrix.dot(probabilities)\n```\n:::\n\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCost of deciding on SVM: 1.9586\n\nCost of deciding on RF: 0.0296\n```\n:::\n:::\n\n\nThe lowest cost would determine the optimal decision.\nWe see that we would incur a significant cost in choosing the SVM over the RF. \nOf course we could have made things even more extreme by having specified more aggressive costs.\nAnd further, we could have extended the possibilities, e.g. adding a row for \"no decision made\".\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}